{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bf97d4-61b9-4f0e-8d06-ef48ba40dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/cagri/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/cagri/.local/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/cagri/.local/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: joblib in /home/cagri/.local/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/cagri/.local/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cagri/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cagri/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/cagri/.local/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cagri/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/cagri/.local/lib/python3.12/site-packages (from sentence-transformers) (4.52.3)\n",
      "Requirement already satisfied: tqdm in /home/cagri/.local/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/cagri/.local/lib/python3.12/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/cagri/.local/lib/python3.12/site-packages (from sentence-transformers) (0.31.4)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/cagri/.local/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/cagri/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cagri/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/cagri/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cagri/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/cagri/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/cagri/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/cagri/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cagri/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/cagri/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/cagri/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/cagri/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cagri/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cagri/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cagri/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cagri/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cagri/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn sentence-transformers joblib --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbaa5e9-d399-49ed-a10d-cceedb1f1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5178\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Data preparation\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def parse_reviews_to_dataframe(dataset_dir):\n",
    "    records = []\n",
    "\n",
    "    for year_dir in os.listdir(dataset_dir):\n",
    "        year_path = os.path.join(dataset_dir, year_dir)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "\n",
    "        review_dir = os.path.join(year_path, f\"{year_dir}_review\")\n",
    "        if not os.path.exists(review_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(review_dir):\n",
    "            if not (fname.endswith(\".json\") and \"ICLR\" in fname):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(review_dir, fname)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            paper_id = data.get(\"id\", fname.replace(\".json\", \"\"))\n",
    "            meta_review = data.get(\"metaReview\", \"\")\n",
    "            reviews = data.get(\"reviews\", [])\n",
    "\n",
    "            review_texts = []\n",
    "            rating_scores = []\n",
    "\n",
    "            for review in reviews:\n",
    "                review_text = review.get(\"review\", \"\")\n",
    "                rating_raw = review.get(\"rating\", \"\")\n",
    "                try:\n",
    "                    rating_score = int(rating_raw.split(\":\")[0].strip())\n",
    "                    rating_scores.append(rating_score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Invalid rating. Error is e: {e}\")\n",
    "\n",
    "                review_texts.append(review_text)\n",
    "\n",
    "            #full_text = \" \".join(review_texts + [meta_review]).strip()\n",
    "            avg_rating = sum(rating_scores) / len(rating_scores)\n",
    "            label = 1 if avg_rating >= 6 else 0\n",
    "\n",
    "            records.append({\n",
    "                \"paper_id\": paper_id,\n",
    "                \"text\": meta_review,\n",
    "                \"avg_rating\": avg_rating,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df = parse_reviews_to_dataframe(\"dataset\")  # <- Your dataset folder path\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9c093d-1763-48f9-bc3b-af10c9dd84fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Class Baseline Accuracy: 0.6737\n"
     ]
    }
   ],
   "source": [
    "# Majority Class\n",
    "majority_label = train_df[\"label\"].mode()[0]\n",
    "majority_preds = [majority_label] * len(test_df)\n",
    "majority_acc = accuracy_score(test_df[\"label\"], majority_preds)\n",
    "print(f\"Majority Class Baseline Accuracy: {majority_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea533f5-ffb7-4de5-920a-acb0ff02b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training TF-IDF + Logistic Regression baseline...\n",
      "TF-IDF + Logistic Regression Accuracy: 0.7703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       349\n",
      "           1       0.82      0.38      0.52       169\n",
      "\n",
      "    accuracy                           0.77       518\n",
      "   macro avg       0.79      0.67      0.68       518\n",
      "weighted avg       0.78      0.77      0.74       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + Logistic Regression\n",
    "print(\"\\n Training TF-IDF + Logistic Regression baseline...\")\n",
    "tfidf_model = make_pipeline(\n",
    "    TfidfVectorizer(max_features=10000),\n",
    "    LogisticRegression(max_iter=1000, random_state=SEED)\n",
    ")\n",
    "tfidf_model.fit(train_df[\"text\"], train_df[\"label\"])\n",
    "tfidf_preds = tfidf_model.predict(test_df[\"text\"])\n",
    "tfidf_acc = accuracy_score(test_df[\"label\"], tfidf_preds)\n",
    "print(f\"TF-IDF + Logistic Regression Accuracy: {tfidf_acc:.4f}\")\n",
    "print(classification_report(test_df[\"label\"], tfidf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f63a048-68c2-4e08-9754-387bf297b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Computing SBERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cagri/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358bfa5f455245a2809877cafab16650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3025225ade3841fcbfc8550a5d12e42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT + Logistic Regression Accuracy: 0.7008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.94      0.81       349\n",
      "           1       0.63      0.20      0.30       169\n",
      "\n",
      "    accuracy                           0.70       518\n",
      "   macro avg       0.67      0.57      0.56       518\n",
      "weighted avg       0.68      0.70      0.64       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentence-BERT + Logistic Regression\n",
    "print(\"\\n Computing SBERT embeddings...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_train = sbert.encode(train_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "X_test = sbert.encode(test_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "clf.fit(X_train, train_df[\"label\"])\n",
    "sbert_preds = clf.predict(X_test)\n",
    "sbert_acc = accuracy_score(test_df[\"label\"], sbert_preds)\n",
    "print(f\"SBERT + Logistic Regression Accuracy: {sbert_acc:.4f}\")\n",
    "print(classification_report(test_df[\"label\"], sbert_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e31de74-3b02-470f-9391-edce8a4f3926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SBERT embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e184e787c859493a8dc317ac4d60509a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b56775861f4a9591f0041889ad1990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65603392e3d347848a975edaa1c791d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP on SBERT embeddings...\n",
      "Epoch 1 | Loss: 81.4076 | Validation Accuracy: 0.6737\n",
      "Epoch 2 | Loss: 76.7239 | Validation Accuracy: 0.6718\n",
      "Epoch 3 | Loss: 73.4870 | Validation Accuracy: 0.6873\n",
      "Epoch 4 | Loss: 71.0482 | Validation Accuracy: 0.6815\n",
      "Epoch 5 | Loss: 68.8346 | Validation Accuracy: 0.6911\n",
      "Epoch 6 | Loss: 66.6963 | Validation Accuracy: 0.7008\n",
      "Epoch 7 | Loss: 64.5903 | Validation Accuracy: 0.6931\n",
      "Epoch 8 | Loss: 62.2848 | Validation Accuracy: 0.6969\n",
      "\n",
      "Evaluating MLP on SBERT embeddings...\n",
      "SBERT + MLP Accuracy: 0.6950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.91      0.80       349\n",
      "           1       0.58      0.25      0.35       169\n",
      "\n",
      "    accuracy                           0.69       518\n",
      "   macro avg       0.64      0.58      0.57       518\n",
      "weighted avg       0.67      0.69      0.65       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# added one more linear classifier as suggested\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\nComputing SBERT embeddings...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Compute SBERT embeddings\n",
    "X_train = sbert.encode(train_df[\"text\"].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
    "X_val = sbert.encode(val_df[\"text\"].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
    "X_test = sbert.encode(test_df[\"text\"].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = torch.tensor(le.fit_transform(train_df[\"label\"].tolist()), dtype=torch.long)\n",
    "y_val = torch.tensor(le.transform(val_df[\"label\"].tolist()), dtype=torch.long)\n",
    "y_test = torch.tensor(le.transform(test_df[\"label\"].tolist()), dtype=torch.long)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define MLP Classifier\n",
    "class MLPClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim=384, hidden_dim=128, num_classes=2):  # 384 for MiniLM\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPClassifier(input_dim=X_train.shape[1], num_classes=len(le.classes_)).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# I tried learning rates of 1e-3 and 1e-4. I got better results with 1e-3   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining MLP on SBERT embeddings...\")\n",
    "model.train()\n",
    "# overfits after 8 epochs\n",
    "for epoch in range(8):\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val.to(device))\n",
    "        val_preds = val_logits.argmax(dim=1).cpu()\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating MLP on SBERT embeddings...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test.to(device)).argmax(dim=1).cpu().numpy()\n",
    "\n",
    "sbert_mlp_acc = accuracy_score(test_df[\"label\"], preds)\n",
    "print(f\"SBERT + MLP Accuracy: {sbert_mlp_acc:.4f}\")\n",
    "print(classification_report(test_df[\"label\"], preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
